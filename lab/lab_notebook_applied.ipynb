{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "bat"
        },
        "id": "ZWfONFvoL_7v"
      },
      "source": [
        "# Hedonic Pricing\n",
        "\n",
        "We often try to predict the price of an asset from its observable characteristics. This is generally called **hedonic pricing**: How do the unit's characteristics determine its market price?\n",
        "\n",
        "In the lab folder, there are three options: housing prices in pierce_county_house_sales.csv, car prices in cars_hw.csv, and airbnb rental prices in airbnb_hw.csv. If you know of another suitable dataset, please feel free to use that one.\n",
        "\n",
        "1. Clean the data and perform some EDA and visualization to get to know the data set.\n",
        "2. Transform your variables --- particularly categorical ones --- for use in your regression analysis.\n",
        "3. Implement an ~80/~20 train-test split. Put the test data aside.\n",
        "4. Build some simple linear models that include no transformations or interactions. Fit them, and determine their RMSE and $R^2$ on the both the training and test sets. Which of your models does the best?\n",
        "5. Make partial correlation plots for each of the numeric variables in your model. Do you notice any significant non-linearities?\n",
        "6. Include transformations and interactions of your variables, and build a more complex model that reflects your ideas about how the features of the asset determine its value. Determine its RMSE and $R^2$ on the training and test sets. How does the more complex model your build compare to the simpler ones?\n",
        "7. Summarize your results from 1 to 6. Have you learned anything about overfitting and underfitting, or model selection?\n",
        "8. If you have time, use the sklearn.linear_model.Lasso to regularize your model and select the most predictive features. Which does it select? What are the RMSE and $R^2$? We'll cover the Lasso later in detail in class."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#1\n",
        "df = pd.read_csv('linearModels/lab/data/airbnb_hw.csv')\n",
        "# drop rows with NaN values\n",
        "df = df.dropna()\n",
        "# Remove '$' and ',' then convert to float\n",
        "df.loc[:, 'Price'] = df['Price'].replace('[\\$,]', '', regex=True).astype(float)\n",
        "\n",
        "#2\n",
        "# Selecting relevant numeric and categorical variables\n",
        "numeric_columns = ['Host Id','Review Scores Rating (bin)','Zipcode','Beds','Number of Records','Number Of Reviews','Review Scores Rating']\n",
        "categorical_columns = ['Neighbourhood ','Property Type', 'Room Type']\n",
        "\n",
        "y = df['Price']\n",
        "X = df[numeric_columns + categorical_columns].copy()\n",
        "# Maxmin normalization function; Sci-kit calls it the \"standard scaler\" (changed for constant values)\n",
        "def maxmin(z):\n",
        "    return (z - z.min()) / (z.max() - z.min()) if z.min() != z.max() else z\n",
        "\n",
        "X[numeric_columns] = X[numeric_columns].apply(maxmin) # Normalize X\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "X = pd.get_dummies(X, columns=categorical_columns, drop_first=True)\n",
        "#3\n",
        "# Split the sample:\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, # Feature and target variables\n",
        "                                                    test_size=0.2, # Split the sample 80 train/ 20 test\n",
        "                                                    random_state=65) # For replication purposes\n",
        "\n",
        "#4\n",
        "train_reg = LinearRegression(fit_intercept=False).fit(X_train, y_train) # Fit the linear model\n",
        "y_train_pred = train_reg.predict(X_train)\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "print(f\"Train RMSE: {train_rmse}\")\n",
        "print(f\"Train R squared measure:{train_reg.score(X, y)}\") # R squared measure\n",
        "\n",
        "test_reg = LinearRegression(fit_intercept=False).fit(X_test, y_test) # Fit the linear model\n",
        "y_test_pred = test_reg.predict(X_test)\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "print(f\"Test RMSE: {test_rmse}\")\n",
        "print(f\"Test R squared measure:{test_reg.score(X, y)}\") # R squared measure\n",
        "\n",
        "#The test values did better, as the RMSE is lower. This means there is less of an error. The difference in the R squared values are negligible.\n"
      ],
      "metadata": {
        "id": "K5bQ-XzdMXSH",
        "outputId": "58ffbfc1-a5cc-4b95-9b93-31c7fd3370aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train RMSE: 133.2070273711007\n",
            "Train R squared measure:0.26160889247882957\n",
            "Test RMSE: 102.98672612727043\n",
            "Test R squared measure:0.25292558891740324\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}